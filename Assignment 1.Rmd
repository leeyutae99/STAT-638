---
title: "Assignment-1(STAT 638)"
author: "Yutae Lee (626005947)"
date: '2022-09-06'
output:
  html_document: default

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 2.1



**Marginal and conditional probability: The social mobility data from Section 2.5 gives a joint probability distribution on (Y1, Y2)= (father’s occupation, son’s occupation). Using this joint distribution, calculate the following distributions:**


**a) the marginal probability distribution of a father’s occupation;**


In order to get marginal probability of father's occupation, you need to get the summation of son's occupation for each margin of father's occupation.
```{r}
farm = 0.018 + 0.035 + 0.031 + 0.008 + 0.018
operatives = 0.002 + 0.112 + 0.064 + 0.032 + 0.069
craftsmen = 0.001 + 0.066 + 0.094 + 0.032 + 0.084
sales = 0.001 + 0.018 + 0.019 + 0.010 + 0.051
professional = 0.001 + 0.029 + 0.032 + 0.043 + 0.130

cat('Marginal Probability of the Father:\n\n')
cat('Farm:', farm ,'\nOperatives:',operatives,'\nCraftsmen:',craftsmen,'\nSales:',sales,'\nProfessional:', professional)



```
**b) the marginal probability distribution of a son’s occupation;**



In order to get marginal probability of son's occupation, you need to get the summation of father's occupation for each margin of son's occupation.
```{r}
farm = 0.018 + 0.002 + 0.001 + 0.001 + 0.001
operatives = 0.035 + 0.112 + 0.066 + 0.018 + 0.029
craftsmen = 0.031 + 0.064 + 0.094 + 0.019 + 0.032
sales = 0.008 + 0.032 + 0.032 + 0.010 + 0.043
professional = 0.018 + 0.069 + 0.084 + 0.051 + 0.130


cat('Marginal Probability of the Father:\n\n')
cat('Farm:', farm ,'\nOperatives:',operatives,'\nCraftsmen:',craftsmen,'\nSales:',sales,'\nProfessional:', professional)
```

**c) the conditional distribution of a son’s occupation, given that the father is a farmer;**


In order to find the conditional distribution of son given that the father is a farmer, we need to first add up the marginal probability of which the father's occupation is farm:

```{r}
f_farm = 0.018 + 0.035 + 0.031 + 0.008 + 0.018
```

Then divide each son's marginal occupation where father's occupation is farm by the total marginal probability of the father's occupation (farm)

```{r}
son_farm_father_farm = 0.018
son_operatives_father_farm = 0.035
son_craftsmen_father_farm = 0.031
son_sales_father_farm = 0.008
son_professional_father_farm = 0.018
cond_farm = son_farm_father_farm/f_farm
cond_operatives = son_operatives_father_farm/f_farm
cond_craftsmen = son_craftsmen_father_farm/f_farm
cond_sales = son_sales_father_farm/f_farm
cond_professional = son_professional_father_farm/f_farm

cat('Conditional Probability of the Son (given that father is farmer):\n\n')
cat('Farm:', cond_farm ,'\nOperatives:',cond_operatives,'\nCraftsmen:',cond_craftsmen,'\nSales:',cond_sales,'\nProfessional:', cond_professional)

```

**d) the conditional distribution of a father’s occupation, given that the son is a farmer.**




In order to find the conditional distribution of father given that the son is a farmer, we need to first add up the marginal probability of which the son's occupation is farm:

```{r}
s_farm = 0.018+0.002+0.001+0.001+0.001
```

Then divide each father's marginal occupation where father's occupation is farm by the total marginal probability of the son's occupation (farm)

```{r}
son_farm_father_farm = 0.018
son_farm_father_operatives = 0.002
son_farm_father_craftsmen = 0.001
son_farm_father_sales = 0.001
son_farm_father_professional = 0.001
cond_farm = son_farm_father_farm/s_farm
cond_operatives = son_farm_father_operatives/s_farm
cond_craftsmen = son_farm_father_craftsmen/s_farm
cond_sales = son_farm_father_sales/s_farm
cond_professional = son_farm_father_professional/s_farm

cat('Conditional Probability of the Father (given that son is farmer):\n\n')
cat('Farm:', cond_farm ,'\nOperatives:',cond_operatives,'\nCraftsmen:',cond_craftsmen,'\nSales:',cond_sales,'\nProfessional:', cond_professional)


```


## Problem 2.2 
**Expectations and variances: Let Y1 and Y2 be two independent random variables, such that** $E[Y_i] = \mu_i$ **and** $Var[Y_i] = \sigma^2_i$.


**Using the definition of expectation and variance, compute the following quantities, where a1  and a2 are given constants:**

**a)** $E[a_1Y_1 + a_2Y_2]$ , $Var[a_1Y_1 + a_2Y_2]$;

First let's consider $E[a_1Y_1]$ alone. By the formula of expectation $E[a_1Y_1] = \sum {a_1y_1p(y_1)} = a_1(\sum{y_1p(y_1)}) = a_1E[Y_1]$ when discrete and $E[a_1Y_1] = \int {a_1y_1p(y_1)} = a_1(\int{y_1p(y_1)}) = a_1E[Y_1]$ when continuous.

In any case we can tell that $E[a_1Y_1] = a_1E[Y_1]$ pretty simply. <- #1

Now consider $E[Y_1 + Y_2]$. By the formula $E[Y_1 + Y_2] = \sum {y_1p(y_1) + y_2p(y_2)} = \sum {y_1p(y_1)}  + \sum {y_2p(y_2)} = E[Y_1] + E[Y_2]$ when discrete and $E[Y_1 + Y_2] = \int {y_1p(y_1) + y_2p(y_2)} = \int {y_1p(y_1)}  + \int {y_2p(y_2)} = E[Y_1] + E[Y_2]$ when continuous.

In any case we can tell that $E[Y_1 + Y_2] = E[Y_1] + E[Y_2]$. <- #2


**Using the two properties above (#1,#2) we can say that** $E[a_1Y_1 + a_2Y_2] = a_1E[Y_1]  + a_2E[Y_2] = a_1\mu_1+ a_2\mu_2$


Now let's find $Var[a_1Y_1 + a_2Y_2]$.

Using the formula:
$$Var[a_1Y_1 + a_2Y_2] = E[(a_1Y_1 + a_2Y_2)^2] - E[a_1Y_1 + a_2Y_2]^2$$ =
$$E[a_1^2Y_1^2+2a_1a_2Y_1Y_2+a_2^2Y_2^2] +E[a_1Y_1]^2+2E[a_1Y_1]E[a_2Y_2] + E[a_2Y_2]^2$$ = 
$$a_1^2E[Y_1^2]+2a_1a_2E[Y_1Y_2]+a_2^2E[Y_2^2]+a_1^2E[Y_1]^2+2a_1a_2E[Y_1]E[Y_2]+a_2^2E[Y_2]^2$$ = 
$$a_1^2(E[Y_1^2] + E[Y_1]^2) + a_2^2(E[Y_2^2] + E[Y_2]^2) + 2a_1a_2(E[Y_1Y_2] + E[Y_1]E[Y_2]) $$ = 
$$a_1^2\sigma^2_1 + a_2^2\sigma^2_2 + 2a_1a_2cov(Y1,Y2)$$
Since Y1 and Y2 are independent covariance has to be 0

So $Var[a_1Y_1 + a_2Y_2] = a_1^2\sigma^2_1 + a_2^2\sigma^2_2$.




**b)** $E[a_1Y_1 − a_2Y_2]$ , $Var[a_1Y_1 − a_2Y_2]$.

Consider $E[Y_1 - Y_2]$. By the formula $E[Y_1 - Y_2] = \sum {y_1p(y_1) - y_2p(y_2)} = \sum {y_1p(y_1)}  - \sum {y_2p(y_2)} = E[Y_1] - E[Y_2]$ when discrete and $E[Y_1 - Y_2] = \int {y_1p(y_1) - y_2p(y_2)} = \int {y_1p(y_1)}  - \int {y_2p(y_2)} = E[Y_1] - E[Y_2]$ when continuous.

In any case we can tell that $E[Y_1 - Y_2] = E[Y_1] - E[Y_2]$.

**Using this fact and the fact from the a) we can say that** $E[a_1Y_1 - a_2Y_2] = a_1E[Y_1]  - a_2E[Y_2] = a_1\mu_1 - a_2\mu_2$


Using the formula:
$$Var[a_1Y_1 - a_2Y_2] = E[(a_1Y_1 - a_2Y_2)^2] - E[a_1Y_1 - a_2Y_2]^2$$ = $$E[a_1^2Y_1^2-2a_1a_2Y_1Y_2+a_2^2Y_2^2] +E[a_1Y_1]^2-2E[a_1Y_1]E[a_2Y_2] + E[a_2Y_2]^2$$ =  $$a_1^2E[Y_1^2]-2a_1a_2E[Y_1Y_2]+a_2^2E[Y_2^2]+a_1^2E[Y_1]^2-2a_1a_2E[Y_1]E[Y_2]+a_2^2E[Y_2]^2$$ =  $$a_1^2(E[Y_1^2] + E[Y_1]^2) + a_2^2(E[Y_2^2] + E[Y_2]^2) - 2a_1a_2(E[Y_1Y_2] + E[Y_1]E[Y_2])$$ =  $$a_1^2\sigma^2_1 + a_2^2\sigma^2_2 + 2a_1a_2cov(Y1,Y2)$$

Since Y1 and Y2 are independent covariance has to be 0

So $Var[a_1Y_1 - a_2Y_2] = a_1^2\sigma^2_1 + a_2^2\sigma^2_2$.

## Problem 2.3 

**Full conditionals: Let X, Y, Z be random variables with joint density (discrete or continuous) p(x, y, z)** $\propto$ **f(x, z)g(y, z)h(z). Show that**

**a) p(x|y, z)** $\propto$ **f(x, z), i.e. p(x|y, z) is a function of x and z;**

$p(x|y,z)$ =  $\frac{p(x,y,z)}{p(y,z)}$ = $\frac{cf(x,z)g(y,z)h(z)}{p(y,z)}$ where c is just a constant (because we know that $p(x,y,z)\)propto f(x,z)g(y,z)h(z)$) since g(y,z) is just a function of y,z, $\frac{cf(x,z)g(y,z)h(z)}{p(y,z)} = \frac{cf(x,z)g(y,z)h(z)}{g(y,z)} = cf(x,z)h(z)$ we can now tell that this is just a function x,z which means **p(x|y, z)** $\propto$ **f(x, z)**


**b) p(y|x, z)** $\propto$ **g(y, z), i.e. p(y|x, z) is a function of y and z;**

$p(y|x,z)$ =  $\frac{p(y,x,z)}{p(x,z)}$ = $\frac{cf(x,z)g(y,z)h(z)}{p(x,z)}$ where c is just a constant (because we know that $p(x,y,z)\propto f(x,z)g(y,z)h(z)$) since f(x,z) is just a function of x,z, $\frac{cf(x,z)g(y,z)h(z)}{p(x,z)} = \frac{cf(x,z)g(y,z)h(z)}{f(x,z)} = cg(y,z)h(z)$ we can now tell that this is just a function x,z which means **p(y|x, z)** $\propto$ **g(y, z)**


**c) X and Y are conditionally independent given Z.**

To prove the conditional independence we need to prove $Pr(X \cap Y|Z) = Pr(X|Z)Pr(Y|Z)$

$p(X,Y|Z) = \frac{p(X,Y,Z)}{p(Z)} = \frac{cf(x,z)g(y,z)h(z)}{p(z)}$ just as a),b) we can say h(z) is just a function of z so $\frac{cf(x,z)g(y,z)h(z)}{p(z)} = \frac{cf(x,z)g(y,z)h(z)}{h(z)} = cf(x,z)g(y,z)$ which we can say that $p(x,y|z) \propto f(x,z)g(y,z)$ choose c = $\frac{1}{p(z)^2}$ then we can easily get $p(x,y|z) = p(x|z)p(y|z)$.



## Problem 2.5


**Urns: Suppose urn H is filled with 40% green balls and 60% red balls, and urn T is filled with 60% green balls and 40% red balls. Someone will flip a coin and then select a ball from urn H or urn T depending on whether the coin lands heads or tails, respectively. Let X be 1 or 0 if the coin lands heads or tails, and let Y be 1 or 0 if the ball is green or red.**


**a) Write out the joint distribution of X and Y in a table.**

Since $P(Y,X) = P(Y|X)P(X)$:


$P(Y =1,X=1)=P(Y=1|X=1)P(X=1)=0.4*0.5=0.2$


$P(Y =1,X=0)=P(Y=1|X=0)P(X=0)=0.6*0.5=0.3$


$P(Y =0,X=1)=P(Y=0|X=1)P(X=1)=0.6*0.5=0.3$


$P(Y =0,X=0)=P(Y=0|X=0)P(X=0)=0.4*0.5=0.2$


Table is below (done through R):
```{r}
x_1 <- c(0.2,0.3)
x_0 <- c(0.3,0.2)
row_names <- c("Y = 1(Green)", "Y = 0 (Red)")
df <- data.frame ("Row Names" = row_names, "X = 1 (Head)" = x_1, "X = 0 (Tails)" = x_0)
print(df)
```

**b) Find** $E[Y]$. **What is the probability that the ball is green?**

We know that:

$E[Y] =  \sum{y_ip(Y = y_i)}$

First we need to find $p(Y = 1)$ and $p(Y = 0)$.

$p(Y=1) = p(Y=1,X=0)+p(Y=1,X=1) = 0.3+0.2 = 0.5$

then $p(Y = 0) = 1-P(Y=1) = 0.5$

**our** $E[Y]$ **would be** $1*0.5+0*0.5 = 0.5$

**The probability for green which is** $p(Y = 1) = 0.5$




**c) Find** $Var[Y |X = 0]$, $Var[Y |X = 1]$ **and** $Var[Y]$. **Thinking of variance as measuring uncertainty, explain intuitively why one of these variances is larger than the others.**

Finding $Var[Y |X = 0]$:



$Var[Y|X=0] = E[Y^2|X=0] - E[Y|X=0]^2$

$E[Y^2|X=0] = E[Y^2 =1 | X = 0] + E[Y^2 =0 | X = 0] = 1^2*0.6 + 0^2*0.4 = 0.6$

$E[Y|X=0]  = E[Y=1|X=0] + E[Y=0|X=0] = 1*0.6 + 0*0.4 = 0.6$

$Var[Y|X=0] = E[Y^2|X=0] - E[Y|X=0]^2 = 0.6 - 0.6^2 = 0.24$


Finding $Var[Y |X = 1]$:



$Var[Y|X=1] = E[Y^2|X=1] - E[Y|X=1]^2$

$E[Y^2|X=1] = E[Y^2 =1 | X = 1] + E[Y^2 =0 | X = 1] = 1^2*0.4 + 0^2*0.6 = 0.4$

$E[Y|X=1]  = E[Y=1|X=1] + E[Y=0|X=1] = 1*0.4 + 0*0.6 = 0.4$

$Var[Y|X=1] = E[Y^2|X=1] - E[Y|X=1]^2 = 0.4 - 0.4^2 = 0.24$

Finding $Var[Y]$:

$Var[Y] = E[Y^2] - E[Y]^2$

We know $E[Y] = 0.5$ from b)

let's find $E[Y^2] = 1^2 * 0.5 +0^2 * 0.5 =0.5$

Thus $Var[Y] = 0.5 - 0.25 = 0.25$.


$Var[Y]$ is bigger than the other two. The reason behind is because unlike the other two $Var[Y]$ does not have conditional statement which does not have any information given.




**d) Suppose you see that the ball is green. What is the probability that the coin turned up tails?**

$P(X = 0|Y = 1) = \frac{P(Y = 1|X = 0) * P(X = 0)}{P(Y = 1)}$ = $\frac{0.3}{0.5} = 0.6$


The probability is 60%
